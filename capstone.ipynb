{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Capstone Project\n",
    "#### Data Acquisition\n",
    "First step is to acquire a large dataset of openly available academic abstracts published in recent years in Computer Science field. \n",
    "For this step, `database.py` Python script  scapes abstract text and the corresponding authors' names from [https://arxiv.org](https://arxiv.org). I loop over publications from February, 2017 back to February, 2012 to collect 5 years worth of abstracts and store the data in sqlite database `Articles` so I would only have to do this once and then work with the database. \n",
    "\n",
    "#### Preprocessing\n",
    "As a standard NLP text cleaning step, I convert the text words to lower case, remove the digits and punctuation and then stem the words using Snowball Stemmer before storing preprocessed text in the database.\n",
    "\n",
    "Working with the Articles databse, `count_db.py` Python script counts the number of publications per author and writes the results in sqlite table `Counts`. Next, I can sort the authors by the number of publications or filter out authors that only have a few publications. \n",
    "\n",
    "I cast the problem as a multiclass classification problem and propose to use a convolutional neural network (CNN) model as a solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data Exploration\n",
    "The full dataset contains 87,587 abstracts from 2/2012 through 2/2017. \n",
    "\n",
    "First, I want to test my convolutional neural network model against benchmark logistic regression model with a much smaller subset of data, using top only authors with 100 or more abstracts. \n",
    "\n",
    "I count the number of articles per author to filter out authors with less than 20 abstracts assuming that 20 publications will be sufficient to train and test the classifier. Overall, there are 107,947 authors with the article count ranging from 166 to 1. If I limit the minimum number of articles per author to 10 then there are 4027 authors. If I limit the minimum number of articles per author to 20 then there are 922 authors.\n",
    "\n",
    "But for simplicity and computational time sake, `cnn.py` Python script limits the number of authors to 10 and compares logistic regression benchmark and CNN models as a proxy to larger number of classes and amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique authors:  10\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect('articles.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# select top authors\n",
    "top_author = 'SELECT author_unique FROM Counts WHERE count >= 50 ORDER BY count DESC LIMIT 10'\n",
    "\n",
    "author_doc = []\n",
    "author_lst = []\n",
    "\n",
    "for i in cur.execute(top_author) :\n",
    "    author_lst.append(i[0])\n",
    "print \"number of unique authors: \", len(author_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I use Pandas library to build a dataframe with one column containing the author and the second column - abstract text. In the database, we might have several authors who collaborated on the publication. So, we want to split the authors and then append the author name and abstract text into a big dataframe. We end up with 1772 abstracts in the dataframe among these top 10 authors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size:  (1172, 2)\n",
      "            author                                                doc\n",
      "0  Rumpe, Bernhard  code generat is regard as an essenti part of m...\n",
      "1  Rumpe, Bernhard  biolog nervous system exhibit astonish complex...\n",
      "2  Rumpe, Bernhard  generat softwar from abstract model is a prime...\n",
      "3  Rumpe, Bernhard  templatebas code generat develop as part of mo...\n",
      "4  Rumpe, Bernhard  mani textual softwar languag share common conc...\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "d={}\n",
    "list_authors = []\n",
    "for i, author_correct in enumerate(author_lst):\n",
    "\n",
    "    cur.execute('''SELECT abstract, author FROM Articles WHERE author LIKE ? ''', ('%{}%'.format(author_correct), ) )\n",
    "    all_rows = cur.fetchall()\n",
    "\n",
    "    authors_list = [x[1] for x in all_rows]\n",
    "    docs = [x[0] for x in all_rows]\n",
    "\n",
    "    for author, row in itertools.izip(authors_list, docs) :\n",
    "        authors = author.split('; ')\n",
    "        for a in authors:\n",
    "            if author_correct == a:\n",
    "                author_doc.append(row)\n",
    "                list_authors.append(a)\n",
    "    conn.commit()\n",
    "\n",
    "df = pd.DataFrame({'author' : list_authors, 'doc': author_doc})\n",
    "print \"size: \", df.shape\n",
    "print df.head()\n",
    "\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can count the number of publications per author to make sure the counts correspond to what we see in the database. As expected, Bernhard Rumpe is the top author with 166 publications and Loet Leydesdorff is the tenth author iwth 96 publications in the database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rumpe, Bernhard          166\n",
      "Bengio, Yoshua           137\n",
      "Poor, H. Vincent         133\n",
      "Zhang, Rui               123\n",
      "Schober, Robert          114\n",
      "Alouini, Mohamed-Slim    106\n",
      "Shen, Chunhua            102\n",
      "Popovski, Petar           98\n",
      "Aickelin, Uwe             97\n",
      "Leydesdorff, Loet         96\n",
      "Name: author, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print df['author'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For CNN model, we want to make sure all the documents are the same length meanining they contain the same amount of words. So, we pad shorter documents to be the same length as the longest document. The longest abstract has 319 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max doc length:  319\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "lengths = []\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    doc = df['doc'][i]\n",
    "    doc_split = doc.split(\" \")\n",
    "    lengths.append(len(doc_split))\n",
    "\n",
    "max_length = max(lengths)\n",
    "print \"max doc length: \", max_length      \n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    doc = df['doc'][i]\n",
    "    doc_split = doc.split(\" \")\n",
    "    # How much to pad each doc\n",
    "    padding_num = max_length - len(doc_split)\n",
    "    doc_new = doc + \" </PAD>\" * padding_num\n",
    "    words = doc_new.split(\" \")\n",
    "    vocab.append(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Instead of dealing with text directly, we want to build a vocabulary of words and then map words to index to convert text to integer vectors for input to CNN model. We first count the words, build an index and then a mapping from index to a word. The size of our vocabulary is the total number of unique words which is 7,247 words. The input data is now converted into integer vectors with the length of 319 for each abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab shape:  7247\n",
      "x size:  (1172, 319)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "word_counts = Counter(itertools.chain(*vocab))\n",
    "\n",
    "# Mapping from index to word\n",
    "vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "\n",
    "# Mapping from word to index\n",
    "vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "\n",
    "# Map docs and labels (authors) to vectors based on a vocabulary\n",
    "print \"vocab shape: \", len(vocabulary)\n",
    "x = np.array([ [ vocabulary[word] for word in doc ] for doc in vocab ])\n",
    "print \"x size: \", x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We want to encode the labels (authors' names) as one-hot vectors to represent true labels in CNN model. First, we convert labels to factors and then use LabelBinarizer function to convert labels to 10-dimensional one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes:  [0 1 2 3 4 5 6 7 8 9]\n",
      "labels:  (1172, 10)\n"
     ]
    }
   ],
   "source": [
    "labels = df['author'].astype('category')\n",
    "labels = labels.cat.codes\n",
    "labels_unique = np.unique(labels)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(labels)\n",
    "print \"classes: \", lb.classes_\n",
    "labels_tf = lb.transform(labels)\n",
    "print \"labels: \", labels_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing data as 2d histograms. For each document, count the frequency of the word and plot as pareto chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code generat is regard as an essenti part of modeldriven develop mdd to systemat transform the abstract model to concret code one current challeng of templatebas code generat is that outputspecif inform ie inform about the generat sourc code is not explicit model and thus not access dure code generat exist approach tri to either pars the generat output or store it in a data structur befor write into a file in this paper we propos a first approach to explicit model part of the generat output these model part are store in a symbol for effici manag dure code generat this inform can be access to ensur that the composit of the overal generat sourc code is valid we achiev this goal by creat a domain model of relev generat output inform extend the symbol tabl to store this inform and adapt the overal code generat process\n",
      "{u'and': 2, u'domain': 1, u'code': 8, u'effici': 1, u'valid': 1, u'creat': 1, u'outputspecif': 1, u'process': 1, u'challeng': 1, u'in': 3, u'thus': 1, u'it': 1, u'mdd': 1, u'an': 1, u'as': 1, u'templatebas': 1, u'exist': 1, u'structur': 1, u'file': 1, u'generat': 10, u'ensur': 1, u'composit': 1, u'systemat': 1, u'are': 1, u'access': 2, u'overal': 2, u'develop': 1, u'goal': 1, u'for': 1, u'ie': 1, u'transform': 1, u'dure': 2, u'by': 1, u'current': 1, u'to': 6, u'concret': 1, u'adapt': 1, u'modeldriven': 1, u'approach': 2, u'is': 4, u'store': 3, u'can': 1, u'a': 5, u'we': 2, u'sourc': 2, u'relev': 1, u'extend': 1, u'that': 2, u'symbol': 2, u'regard': 1, u'tabl': 1, u'paper': 1, u'befor': 1, u'part': 3, u'pars': 1, u'not': 2, u'be': 1, u'one': 1, u'data': 1, u'the': 8, u'tri': 1, u'about': 1, u'essenti': 1, u'this': 4, u'of': 5, u'into': 1, u'these': 1, u'explicit': 2, u'abstract': 1, u'manag': 1, u'inform': 5, u'achiev': 1, u'write': 1, u'either': 1, u'output': 3, u'model': 5, u'first': 1, u'or': 1, u'propos': 1}\n",
      "LABELS:  (u'write', u'we', u'valid', u'tri', u'transform', u'to', u'thus', u'this', u'these', u'the', u'that', u'templatebas', u'tabl', u'systemat', u'symbol', u'structur', u'store', u'sourc', u'relev', u'regard', u'propos', u'process', u'part', u'pars', u'paper', u'overal', u'outputspecif', u'output', u'or', u'one', u'of', u'not', u'modeldriven', u'model', u'mdd', u'manag', u'it', u'is', u'into', u'inform', u'in', u'ie', u'goal', u'generat', u'for', u'first', u'file', u'extend', u'explicit', u'exist', u'essenti', u'ensur', u'either', u'effici', u'dure', u'domain', u'develop', u'data', u'current', u'creat', u'concret', u'composit', u'code', u'challeng', u'can', u'by', u'befor', u'be', u'as', u'are', u'approach', u'and', u'an', u'adapt', u'achiev', u'access', u'abstract', u'about', u'a')\n",
      "heights:  (1, 2, 1, 1, 1, 6, 1, 4, 1, 8, 2, 1, 1, 1, 2, 1, 3, 2, 1, 1, 1, 1, 3, 1, 1, 2, 1, 3, 1, 1, 5, 2, 1, 5, 1, 1, 1, 4, 1, 5, 3, 1, 1, 10, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAEGCAYAAABcolNbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117668210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# # store in data frame\n",
    "    # words as columns, authors as rows\n",
    "    #           |  word 1   |  word 2   | word n\n",
    "    #           _____________________________________\n",
    "    #  author 1 | frequency | frequency | frequency\n",
    "    #  author 2 | frequency | frequency | frequency\n",
    "    #  author 3 | frequency | frequency | frequency   \n",
    "\n",
    "abstract = df.iloc[0]\n",
    "print abstract.values[1]\n",
    "# print type(abstract.values)\n",
    "\n",
    "def counts(text):\n",
    "    counts = dict()\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        counts[word] = counts.get(word,0) + 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "test = counts(abstract.values[1])        \n",
    "print test\n",
    "\n",
    "# sort counts in descending order\n",
    "labels, heights = zip(*sorted(((k, v) for k, v in test.items()), reverse=True))\n",
    "\n",
    "print \"LABELS: \", labels\n",
    "print \"heights: \", heights\n",
    "from paretochart import pareto\n",
    "pareto(heights)\n",
    "plt.title('Basic chart without labels', fontsize=10)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principle Component Analysis to try to visualize the data. The first two PCA components only account for about 6 percent of variance in the data. When visualizing the scatter matrix of the first two components, the points do not seem to cluster together for a particular label but are randomly distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(x)\n",
    "\n",
    "print(pca.explained_variance_ratio_) \n",
    "\n",
    "reduced_data = pca.transform(x)\n",
    "\n",
    "# Create a DataFrame for the reduced data\n",
    "reduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2'])\n",
    "\n",
    "# scatterplot of the reduced data    \n",
    "\n",
    "fig, ax = plt.subplots(figsize = (20,10))\n",
    "ax.scatter(x=reduced_data.loc[:, 'Dimension 1'], y=reduced_data.loc[:, 'Dimension 2'], s=70, alpha=0.5, c=labels)\n",
    "ax.set_xlabel(\"Dimension 1\", fontsize=14)\n",
    "ax.set_ylabel(\"Dimension 2\", fontsize=14)\n",
    "\n",
    "# Set plot title\n",
    "ax.set_title(\"PCA-Reduced Data colored by label\", fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# heatmap\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(x, annot=True, linewidth=.1, vmax=99, fmt='.1f', cmap='YlOrRd', square=True, cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsample x and visualize word counts as heatmap...\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As with any machine learning application, we want to split our data into training and testing datasets. I used StratifiedShuffleSplit to split the data and labels. After splitting, the training set is 1054 samples and the testing set is 118 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=0)\n",
    "for train_index, test_index in sss.split(x, labels_tf):\n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = labels_tf[train_index], labels_tf[test_index]\n",
    "    \n",
    "    # For logistic regression model, we simply need the labels (not one-hot vectors)\n",
    "    labels_train, labels_test = df['author'][train_index], df['author'][test_index]\n",
    "\n",
    "vocab_size = len(vocabulary)\n",
    "doc_size = x_train.shape[1]\n",
    "print \"Train/test split: %d/%d\" % (len(y_train), len(y_test))\n",
    "print 'Train shape:', x_train.shape\n",
    "print 'Test shape:', x_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Benchmark Model\n",
    "For the benchmark model, I chose to build Logistic Regression Model due to its simplicity and easy interpretation. The benchmark perform okay with the accuracy score on the test dataset of 0.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from time import time\n",
    "\n",
    "print \"starting log reg modeling...\"\n",
    "\n",
    "t0 = time()\n",
    "clf = linear_model.LogisticRegression(solver='sag', max_iter=1000, random_state=42,\n",
    "                                 multi_class=\"ovr\").fit(x_train, labels_train)\n",
    "pred = clf.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(pred, labels_test)\n",
    "\n",
    "tt = time()-t0\n",
    "print(\"Training Log Regression took: {}\").format(round(tt,3))\n",
    "print \"Accuracy score on test data is {}.\".format(round(acc,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolutional Neural Network Model\n",
    "I used TensorFlow to build a multi-layer convolutional network. The first layer is embedding layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from time import time\n",
    "\n",
    "num_classes = len(labels_unique)\n",
    "\n",
    "x = tf.placeholder(tf.int32, [None, x_train.shape[1]], name=\"input_x\")\n",
    "\n",
    "# y_ is the correct classes\n",
    "y_ = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "\n",
    "# Keeping track of l2 regularization loss \n",
    "l2_loss = tf.constant(0.0)\n",
    "\n",
    "embedding_size = 128\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 128\n",
    "\n",
    "# Embedding layer\n",
    "W_emb = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0))\n",
    "embedded_chars = tf.nn.embedding_lookup(W_emb, x)\n",
    "embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The second layer is convolution and pooling layer with three different filter lengths of 3, 4, and 5 that are then combined. The pooling filters out the maximum value for each convolution to reduce the size of the layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create a convolution + maxpool layer for each filter size\n",
    "pooled_outputs = []\n",
    "for i, filter_size in enumerate(filter_sizes):\n",
    "    # Convolution Layer\n",
    "    filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "    W_conv = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1))\n",
    "    b_conv = tf.Variable(tf.constant(0.1, shape=[num_filters]))\n",
    "    conv = tf.nn.conv2d(\n",
    "        embedded_chars_expanded,\n",
    "        W_conv,\n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding=\"VALID\")\n",
    "    # Apply nonlinearity\n",
    "    h = tf.nn.relu(tf.nn.bias_add(conv, b_conv))\n",
    "    # Maxpooling over the outputs\n",
    "    pooled = tf.nn.max_pool(\n",
    "        h,\n",
    "        ksize=[1, doc_size - filter_size + 1, 1, 1],\n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding='VALID')\n",
    "    pooled_outputs.append(pooled)\n",
    "\n",
    "# Combine all the pooled features\n",
    "num_filters_total = num_filters * len(filter_sizes)\n",
    "h_pool = tf.concat(3, pooled_outputs)\n",
    "h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To prune the network and reduce overfitting, we add dropout during training that disables the neurons that are less than 0.5 probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Add dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_drop = tf.nn.dropout(h_pool_flat, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The last layer is fully-connected layer with the softmax output classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Unnormalized scores and predictions\n",
    "W = tf.get_variable(\n",
    "    \"W\",\n",
    "    shape=[num_filters_total, num_classes],\n",
    "    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.Variable(tf.constant(0.1, shape=[num_classes]))\n",
    "l2_loss += tf.nn.l2_loss(W)\n",
    "l2_loss += tf.nn.l2_loss(b)\n",
    "scores = tf.nn.xw_plus_b(h_drop, W, b)\n",
    "predictions = tf.argmax(scores, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "During training, we use Adam Optimizer to minimize the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "l2_reg_lambda=0.0\n",
    "\n",
    "losses = tf.nn.softmax_cross_entropy_with_logits(scores, y_)\n",
    "cross_entropy = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "# Accuracy score\n",
    "correct_predictions = tf.equal(predictions, tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Training accuracy is computed for 500 iterations and printed every 100th step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "for i in range(500):\n",
    "    # batch = mnist.train.next_batch(50)\n",
    "    if i%100 == 0:\n",
    "\n",
    "        train_accuracy = accuracy.eval(session=sess, feed_dict={ x: x_train, y_: y_train, keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "    train_step.run(session=sess, feed_dict={x: x_train, y_: y_train, keep_prob: 0.5})\n",
    "\n",
    "\n",
    "tt = time()-t0\n",
    "print(\"Training TensorFlow NN took: {}\").format(round(tt,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The CNN test accuracy is 0.86."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"test accuracy %g\"%accuracy.eval(session=sess, feed_dict={\n",
    "    x: x_test, y_: y_test, keep_prob: 1.0}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
